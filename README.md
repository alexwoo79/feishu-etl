版本的程序可以使用命令输入参数：
它主要用于从飞书多维表格的源表抽取数据，经过转换后加载到目标表，支持全量（full）和增量（incremental）两种运行模式（默认是全量模式）。
其中各参数含义如下：
- -c, --config string：指定配置文件路径，默认是 config.json。
- -d, --dry-run：Dry - run 模式，只打印相关操作但不执行写入操作。
- -h, --help：显示 feishu-etl 工具的帮助信息。
- -m, --mode string：设置运行模式，可选择 full（全量）或 incremental（增量），默认 full。

一、功能

本脚本用于**自动同步飞书多维表格数据**，支持全量和增量模式，将源表（如日报、工时等）数据经过清洗、转换后，批量写入目标表。  
主要功能包括：
- 自动获取飞书 API Token
- 抓取源表数据，支持分页
- 数据清洗（过滤重复、删除、空数据等）
- 数据转换（项目工时拆分、格式标准化）
- 增量去重（根据“填报日期+姓名+项目名称”唯一键）
- 批量写入目标表，支持 dry-run 测试模式
- 支持全量/增量同步，按日期窗口筛选

---
二、使用方法
1. 配置文件
请在 config.json 中填写如下内容：
{
  "app_id": "你的飞书应用ID",
  "app_secret": "你的飞书应用Secret",
  "bitable_app_token": "你的多维表格AppToken",
  "source_table": "源表ID",
  "target_table": "目标表ID",
  "mode": "full",         // "full" 全量同步，"incremental" 增量同步
  "days": 7,              // 增量模式下同步最近多少天
  "batch_size": 50,       // 每批写入条数
  "date_field": "填报日期",// 日期字段名
  "dry_run": false        // true只打印不写入，false实际写入
}

2. 运行脚本
在命令行运行：
python feishu.py

---
三、主要流程说明

1. 获取 API Token自动调用飞书接口获取 tenant_access_token。
  
2. 抓取源表数据支持分页，自动获取所有数据。
  
3. 数据清洗与转换  
  - 过滤掉“检查”列中包含“重复”、“删除”、“空数据”的行。
  - 项目名称与工时合并为“项目1/项目2/项目3”字段。
  - melt展开为多行，每行一个项目。
  - 拆分项目名称和工时，工时转为数值，过滤掉为0或缺失的行。
    
4. 增量去重  
  - 获取目标表已有数据，构建唯一键集合（填报日期+姓名+项目名称）。
  - 只保留源表中目标表没有的新数据。
    
5. 批量写入目标表  
  - 按 batch_size 分批写入。
  - 支持 dry-run 只打印不写入。
6. 支持增量/全量同步  
  - 增量模式下只同步最近 N 天数据。
  - 全量模式同步所有数据。
    

---
四、注意事项
- 源表和目标表字段需保持一致，项目名称/工时字段格式需标准化。
- 若 dry_run 为 true，则不会实际写入目标表，仅打印处理结果。
- 若目标表字段有变动，请同步修改脚本中的字段名。
- 若数据量大，建议适当调大 batch_size。
  

---

五、常见问题
- 字段不匹配报错：请确保目标表字段与脚本一致。
- API Token获取失败：请检查 app_id、app_secret 是否正确。
- 数据分裂报错：请确保项目名称-工时字段格式正确，分隔符一致。
- 无数据写入：可能是增量去重后无新数据，或过滤条件过严。
  

---
六、代码入口
主入口为：

if __name__ == "__main__":
    cfg = load_config("config.json")
    etl_pipeline(cfg)

---
如需定制化同步逻辑或字段，请根据实际业务需求调整 transform_source_records 等相关函数。
